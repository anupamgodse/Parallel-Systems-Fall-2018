Usage:

To generate executable my_rtt :

>>make -f p1.Makefile

Run:

>>./my_prun ./my_rtt

Results:

Output will be generated in a output.txt file.

Output format: Size	Pair	Avg.RTT	Std.Deviation


Implementation details:

MPI_Init():

When a process will call MPI_Init it will create a socket and start listening to the socket.
All processes will act server,

Also it will write its own ip address and port number to a file which other processes can read
when they have to communicate with this process.
The process after writing its ip and port number will read all other processe's ip and port number
from other processes files and store it in array all_addr[].
Before reading all processes will synchronize by creating a file and polling to check if all other 
processes have created a file.


MPi_Send():

The process will act as a client and open a new socket. It will take the destination ip address and 
port from any_addr array and will connect() to the destination process and then write() from sendbuf 
until all data is written.

This function does not buffer any data.

MPI_Recv():

The Process will accept any incoming connections and read() from new_sock_fd into the receive buffer
until add expected data is read.

This function does not buffer any data.

MPI_Gather():

The root process will wait to receive data using MPI_Recv() from all other processes in a loop
into the appropriate slot of receive buffer. 
It will also copy its own data into the appropriate slot of receive buffer.
After receiveng data from all the processes, root will send acknowledgement to all.

All other processes will send to root using MPI_Recv and wait from ackowlegement(MPI_Recv) from root.


MPI_Barrier():

The 0th process will wait to receive from all the processes.
All the processes will send to 0th process and wait to get ack from 0th process.
The 0th process will send ack to all the processes.



Comments:

Works for HW1 problem 2 without making any changes.
None of the function uses tag and the running is successfull only because there is particular calling sequence of 
MPI_Send, MPI_Recv and MPI_Gather. 
Uses file creation and polling in MPI_Init and MPI_Send.

Future Scope:
This solution works great for HW1 problem 2. 
To further generatlize the use buffering can be used in MPI_Send and MPI_Recv to store the data which is not yet 
demanded by any process.



Comparing results with HW1 problem 2:

Results for HW1 problem 2 with standard MPI library

Size	Pair	Avg. RTT	Std. Deviation
32	1	0.00007355555556	0.00018298154801
32	2	0.00001800000000	0.00003121609272
32	3	0.00003811111111	0.00008311363633
32	4	0.00002588888889	0.00005595721470
64	1	0.00000644444444	0.00000068493489
64	2	0.00000700000000	0.00000047140452
64	3	0.00000633333333	0.00000105409255
64	4	0.00000700000000	0.00000047140452
128	1	0.00000655555556	0.00000095581392
128	2	0.00000711111111	0.00000087488976
128	3	0.00000988888889	0.00000959681017
128	4	0.00000700000000	0.00000094280904
256	1	0.00000966666667	0.00000473755680
256	2	0.00000933333333	0.00000047140452
256	3	0.00000844444444	0.00000049690399
256	4	0.00000933333333	0.00000115470054
512	1	0.00000900000000	0.00000081649658
512	2	0.00001144444444	0.00000411261234
512	3	0.00000933333333	0.00000047140452
512	4	0.00001255555556	0.00000725888132
1024	1	0.00001133333333	0.00000047140452
1024	2	0.00001200000000	0.00000066666667
1024	3	0.00001122222222	0.00000041573971
1024	4	0.00001177777778	0.00000062853936
2048	1	0.00001466666667	0.00000105409255
2048	2	0.00001588888889	0.00000087488976
2048	3	0.00001511111111	0.00000166295884
2048	4	0.00001533333333	0.00000066666667
4096	1	0.00002155555556	0.00000901986833
4096	2	0.00002077777778	0.00000131468440
4096	3	0.00001900000000	0.00000066666667
4096	4	0.00002000000000	0.00000066666667
8192	1	0.00010777777778	0.00016747588734
8192	2	0.00002311111111	0.00000056655772
8192	3	0.00002655555556	0.00000444999653
8192	4	0.00002344444444	0.00000049690399
16384	1	0.00003611111111	0.00000152347880
16384	2	0.00003588888889	0.00000435748257
16384	3	0.00003666666667	0.00000487624628
16384	4	0.00003500000000	0.00000094280904
32768	1	0.00005366666667	0.00000618241233
32768	2	0.00008277777778	0.00008919364424
32768	3	0.00005122222222	0.00000131468440
32768	4	0.00010200000000	0.00014425209415
65536	1	0.00008400000000	0.00000501109879
65536	2	0.00008211111111	0.00000087488976
65536	3	0.00011500000000	0.00008365670591
65536	4	0.00008244444444	0.00000083147942
131072	1	0.00014388888889	0.00000428030229
131072	2	0.00014611111111	0.00000073702773
131072	3	0.00014522222222	0.00000113311545
131072	4	0.00014666666667	0.00000336650165
262144	1	0.00026544444444	0.00000068493489
262144	2	0.00027333333333	0.00000115470054
262144	3	0.00027188888889	0.00000223330569
262144	4	0.00027288888889	0.00000099380799
524288	1	0.00051188888889	0.00000109994388
524288	2	0.00052833333333	0.00000169967317
524288	3	0.00052455555556	0.00000416629628
524288	4	0.00052622222222	0.00000154759870
1048576	1	0.00100277777778	0.00000103040206
1048576	2	0.00103755555556	0.00000125707872
1048576	3	0.00102744444444	0.00000240883149
1048576	4	0.00103355555556	0.00000177081972
2097152	1	0.00198988888889	0.00001254719486
2097152	2	0.00205744444444	0.00000298556197
2097152	3	0.00203400000000	0.00000133333333
2097152	4	0.00204688888889	0.00000409456128


Results for HW1 problem 2 with my_mpi library 

Size	Pair	Avg. RTT	Std. Deviation
32	1	0.00070411111111	0.00004796861114
32	2	0.00048433333333	0.00002909753712
32	3	0.00071300000000	0.00006060436361
32	4	0.00049200000000	0.00002795234039
64	1	0.00070833333333	0.00003307902995
64	2	0.00062355555556	0.00004637315102
64	3	0.00074244444444	0.00004119630812
64	4	0.00062422222222	0.00007099886975
128	1	0.00072033333333	0.00002421202640
128	2	0.00059644444444	0.00002294975725
128	3	0.00073366666667	0.00004702717890
128	4	0.00059055555556	0.00003556874755
256	1	0.00072822222222	0.00004452159970
256	2	0.00060266666667	0.00001886796226
256	3	0.00071155555556	0.00004571192675
256	4	0.00060788888889	0.00006308978513
512	1	0.00070522222222	0.00006194222717
512	2	0.00060400000000	0.00003079321715
512	3	0.00076722222222	0.00006058195143
512	4	0.00060477777778	0.00003652328870
1024	1	0.00070777777778	0.00004214466561
1024	2	0.00061622222222	0.00004202057109
1024	3	0.00070166666667	0.00007802706083
1024	4	0.00060511111111	0.00003564467997
2048	1	0.00075622222222	0.00006047180753
2048	2	0.00066400000000	0.00008320389948
2048	3	0.00078288888889	0.00005743101068
2048	4	0.00066288888889	0.00002189086286
4096	1	0.00085600000000	0.00007738934753
4096	2	0.00078811111111	0.00012774927218
4096	3	0.00088977777778	0.00009809040929
4096	4	0.00081922222222	0.00013416886357
8192	1	0.00086166666667	0.00011167810887
8192	2	0.00086266666667	0.00006951258879
8192	3	0.00091233333333	0.00005541560149
8192	4	0.00087877777778	0.00009628750672
16384	1	0.00128544444444	0.00006254440398
16384	2	0.00119988888889	0.00011093185540
16384	3	0.00129955555556	0.00011911815900
16384	4	0.00121488888889	0.00014629987806
32768	1	0.00159844444444	0.00020838058723
32768	2	0.00156255555556	0.00021480901652
32768	3	0.00159377777778	0.00021776020337
32768	4	0.00153744444444	0.00016058511839
65536	1	0.00271377777778	0.00035576014947
65536	2	0.00233555555556	0.00027376068670
65536	3	0.00264277777778	0.00027363348470
65536	4	0.00241944444444	0.00014369807167
131072	1	0.00465222222222	0.00024559713072
131072	2	0.00416933333333	0.00022229809816
131072	3	0.00440944444444	0.00021451553226
131072	4	0.00447233333333	0.00025734110524
262144	1	0.00784755555556	0.00017255022529
262144	2	0.00722088888889	0.00017455813705
262144	3	0.00788022222222	0.00022368518442
262144	4	0.00767277777778	0.00028395922156
524288	1	0.01300888888889	0.00029990533486
524288	2	0.01196066666667	0.00049361388419
524288	3	0.01336877777778	0.00055655463499
524288	4	0.01269644444444	0.00040485665807
1048576	1	0.02007688888889	0.00055644355697
1048576	2	0.01927177777778	0.00047256975447
1048576	3	0.01976600000000	0.00068172102473
1048576	4	0.01976211111111	0.00078400133850
2097152	1	0.03794333333333	0.00096418900867
2097152	2	0.03704055555556	0.00029729675078
2097152	3	0.03723188888889	0.00065947579257
2097152	4	0.03728322222222	0.00068805664783 


We can see that out implementation is approximately 20 times slower
than the standard MPI library implementaion.
This may be because of various reasons like, the way mpi passes data between 
processes, the type of hierarchy it uses for communication (mainly for gather) (example tree),
the synchronization techniques used and the buffering technique.

